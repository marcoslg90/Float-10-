{"cells":[{"cell_type":"code","source":["# Directorio en Databricks File System con los archivos a procesar\ndirectorio = \"/FileStore/tables/5qafb5rr1493121316812/\""],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["archivos = dbutils.fs.ls(directorio)\nnombres=[]\nfor archivo in archivos:\n  nombres.append(archivo.name)\nprint nombres"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n\nfrom string import punctuation\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import lit\nfrom datetime import *\nimport pandas as pd\nfrom pyspark import SQLContext\n\nsqlc = SQLContext(sc)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["dataFrame_dicci = sqlContext.sql(\"SELECT * FROM diccionario\")\ndiccionario = dataFrame_dicci.select(\"palabra\").rdd.flatMap(lambda x: x).collect()\n\ndataFrame_stop = sqlContext.sql(\"SELECT * FROM stop\")\nsp_stopwords = dataFrame_stop.select(\"stpword\").rdd.flatMap(lambda x: x).collect()\n\n#Código unicode correspondiente a los caracteres válidos para filtrar el texto.\npuntuacion_si = [32, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 193, 201, 205, 199, 209, 211, 218, 220, 225, 231, 233, 237, 241, 243, 250, 252]"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["###############\n#######Fuciones\n###############\n\n# Filtrado de palabras compuestas o simples que empiecen por mayúscula. \ndef buscarPalabrasCompuestas(texto):\n  mayusculas = [ 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'Ñ', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n  anterior = False\n  palabraaux = \"\"\n  listabpc = texto.split(\".\")\n  compuestas = []\n  for i in listabpc:\n      listabpc2 =i.split(',')\n\n      for j in listabpc2:\n          listabpc3 = j.split(\" \")\n\n          for ind,k in enumerate(listabpc3):\n              \n              if k != \"\":\n              \n                  if k[0] in mayusculas:\n                      if anterior == False:\n\n                          palabraaux += k\n                      else:\n                          palabraaux += \" \"\n                          palabraaux += k\n\n                      anterior = True\n                      if ind == (len(listabpc3)-1):\n                          compuestas.append(palabraaux)\n                          palabraaux = \"\"\n                          anterior = False\n\n                  if k[0] not in mayusculas:\n                      if anterior == True  :\n                          compuestas.append(palabraaux)\n                          palabraaux = \"\"\n                      anterior = False\n\n\n  return compuestas\n\n# Creacion de una lista ordenada con el número de veces que aparece la palabra del diccionario en el texto.\ndef palabrasCompuestas(texto):\n  lista = []\n  for i in diccionario:\n    a = texto.count(i)\n    lista.append(a)   \n  return lista\n\n# Suma de los valores que genera el metodo palabrasCompuestas() para cada una de las palabras del diccionario.\ndef Contar(acum,n):\n  for j in range(len(diccionario)):\n    acum[j] += n[j]\n  return acum\n\n# Limpieza del texto de palabras dentro de diccionario.\ndef eliminarPalabrasComp(texto):\n  for palabra in diccionario:\n    texto = texto.replace(palabra, \"\")\n  return texto\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["##################\n#####Procedimiento\n##################\n\nfor fichero in nombres:\n  \n  mifich = sc.textFile(directorio + fichero)\n  \n  previo = mifich.flatMap(buscarPalabrasCompuestas)\n  \n  previo1 = previo.map(lambda palabra : ''.join([char for char in palabra if ord(char) in puntuacion_si]))\n  \n  previo2 = previo1.map(lambda x : (x,1)).reduceByKey(lambda x,y : x+y).collect()\n  \n  #Creamos un dataframe de pandas vacio que se utlilizara para almacenar temporalmente las palabras de la lista diccionario.\n  df = pd.DataFrame(columns = ['palabra'])\n  #Casting de candidatas a palabras compuestas para introducir en el diccionario.\n  for palabra in previo2:\n    if palabra[1] >= 4 and palabra[0] not in diccionario and palabra[0].lower().encode('utf8') not in list(map(lambda x: x.encode('utf8'), sp_stopwords)):\n      diccionario.append(palabra[0])     \n      data = pd.DataFrame({'palabra':[palabra[0]]})\n      df = df.append(data)\n  \n  if not df.empty:\n    sqlc.createDataFrame(df).write.insertInto(\"diccionario\")\n  \n  lista2 = mifich.map(palabrasCompuestas)\n\n  lista3 = lista2.reduce(Contar) \n  \n  tran1 = mifich.map(eliminarPalabrasComp)\n  \n  tran2 = tran1.map(lambda reg : ''.join([char for char in reg if ord(char) in puntuacion_si])) \n\n  tran3 = tran2.flatMap(lambda reg : reg.split(' ')).map(lambda x : x.lower())\n\n  tran4 = tran3.filter(lambda palabra : palabra not in sp_stopwords)\n\n  tran5 = tran4.map(lambda x : (x,1))\n\n  salida = tran5.reduceByKey(lambda x,y : x+y).toDF(['clave','valor'])\n  \n  datos = fichero.replace(\".txt\",\"\").split(\"_\")\n  fecha = date(int(datos[1]), int(datos[2]), int(datos[3]))\n  fecha = fecha.isocalendar()\n\n  categoria = datos[0]\n  anio = datos[1]\n  mes = datos[2]\n  dia = datos[3]\n  semana = fecha[1]\n  dia_semana = fecha[2]\n  \n\n  lista_comp = []\n  for i in range(len(diccionario)):\n    if lista3[i] != 0:\n      lista_comp.append((diccionario[i],lista3[i]))\n  \n  rdd_comp = sc.parallelize(lista_comp)\n  salida_comp = rdd_comp.toDF(['clave','valor'])\n\n  salida_comp = salida_comp.withColumn(\"categoria\", lit(categoria)).withColumn(\"anio\", lit(anio)).withColumn(\"mes\", lit(mes)).withColumn(\"dia\", lit(dia)).withColumn(\"semana\", lit(semana)).withColumn(\"dia_semana\", lit(dia_semana))\n  \n  salida = salida.withColumn(\"categoria\", lit(categoria)).withColumn(\"anio\", lit(anio)).withColumn(\"mes\", lit(mes)).withColumn(\"dia\", lit(dia)).withColumn(\"semana\", lit(semana)).withColumn(\"dia_semana\", lit(dia_semana))\n\n  \n  salida.write.insertInto(\"contadorDB.palabrastexto\")\n  salida_comp.write.insertInto(\"contadorDB.palabrastexto\")\n  "],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["###############\n##Mapa de Calor\n###############\n\ndef mapaPalabras(texto):\n  principal = False\n  c=0\n\n  posicion1 = 0\n  posicion2 = 0\n  valor = 0.0\n  salida = []\n  texto = texto.split(\" \")\n  for auxiliar in palabra2:\n\n      c=0\n      posicion1 = 0\n      posicion2 = 0\n      valor = 0.0\n      principal = False\n\n      for ind,i in enumerate(texto):\n\n          i = i.replace(\",\",\"\").replace(\".\",\"\")\n\n\n          if i == palabra.decode('utf8'):\n\n              posicion1= ind\n              if principal == False and posicion2 !=0:\n                  c+=1\n                  valor = valor+(posicion1-posicion2)\n              principal = True\n\n\n          if i == auxiliar.decode('utf8') :\n\n              posicion2 = ind\n              if principal == True:\n                  c+=1\n                  valor = valor+(posicion2-posicion1)\n              principal = False\n\n      if c !=0:\n          valor = valor/c\n\n          salida.append((palabra, auxiliar, valor,c))\n\n  return salida\n\n# palabra principal respecto a la que hacer el mapa de calor\npalabra = \"Madrid\"\n\n# lista de palabras con las que se quiere comparar la principal\npalabra2= [\"Granada\", \"Ponferrada\", \"Valencia\", \"Barcelona\", \"Sevilla\",\"Messi\", \"partido\"]\n\nmifich = sc.textFile(directorio + nombres[0])\nsalidaMapa2 = mifich.flatMap(mapaPalabras)\nsalidaMapa3 = salidaMapa2.collect()\n\nsal = salidaMapa3 \n\nlistaMapaAux = []\nminimo = 100000\nmaximo = 0\n\nprint sal\n\nfor i in palabra2:\n    valor=0\n    cantidad = 0\n\n    for lista in sal:\n        if lista !=[]:\n            \n          if lista[1] == i:\n            valor += lista[2]*lista[3]\n            cantidad += lista[3]\n                        \n    if cantidad !=0:\n        listaMapaAux.append((i,valor/cantidad))\n\nfor i in listaMapaAux:\n    if i[1] < minimo:\n        minimo = i[1]\n    if i[1] > maximo:\n        maximo = i[1]\n        \nlistaMapaAux2 = []\nfor i in listaMapaAux:\n    \n    if (maximo-minimo) != 0:\n        listaMapaAux2.append((palabra, i[0], str(int(round(i[1]))),str(int(round(100-((i[1]-minimo)/(maximo-minimo))*100)))))\n    else:\n        listaMapaAux2.append((palabra ,i[0], str(int(round(i[1]))),str(int(round(100-((i[1]-minimo)/(1))*100)))))\n        \nlistaMapaAux2 = sc.parallelize(listaMapaAux2).toDF([\"Clave\",\"Palabra\",\"Distancia\",\"Relación\"])\nlistaMapaAux2.show()"],"metadata":{},"outputs":[],"execution_count":7}],"metadata":{"name":"algoritmoFinalDataBricks","notebookId":358555501291934},"nbformat":4,"nbformat_minor":0}
